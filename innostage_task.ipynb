{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cd2685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "ss = SparkSession \\ \n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c35480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[ \n",
    "    StructField(\"player_id\", IntegerType()),\n",
    "    StructField(\"session_id\", StringType ()),\n",
    "    StructField(\"event\", StringType ())]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab8eceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|player_id|session_id|event|\n",
      "+---------+----------+-----+\n",
      "+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sessions = open(\"sessions.csv\", \"w+\")\n",
    "df = ss.read.csv(\"sessions.csv\", schema=schema, sep=\";\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "87eca084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f78f347c-a9f9-449c-8518-96a157d03958</td>\n",
       "      <td>a310641c-9846-4c5e-811f-0f7ec9d4e79e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e8c49748-9471-4aa1-b215-ca61b93b74c1</td>\n",
       "      <td>af9efe21-57b6-4cc6-8316-2f251be58585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>645f3d8c-1c85-4e90-bb42-ea1d401b6845</td>\n",
       "      <td>e9d0bccb-d016-4dee-be1c-1541b739c47f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9a6a1678-3db0-4cd2-b3b4-29cd41e8ba87</td>\n",
       "      <td>4f57d97a-34d9-44bc-a9a9-2101710e7236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>445013be-57fb-4da3-904d-dae3919068cb</td>\n",
       "      <td>3c280879-e26e-4c37-92fa-413a2a4ebafd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              player_id                            session_id  \\\n",
       "0  f78f347c-a9f9-449c-8518-96a157d03958  a310641c-9846-4c5e-811f-0f7ec9d4e79e   \n",
       "1  e8c49748-9471-4aa1-b215-ca61b93b74c1  af9efe21-57b6-4cc6-8316-2f251be58585   \n",
       "2  645f3d8c-1c85-4e90-bb42-ea1d401b6845  e9d0bccb-d016-4dee-be1c-1541b739c47f   \n",
       "3  9a6a1678-3db0-4cd2-b3b4-29cd41e8ba87  4f57d97a-34d9-44bc-a9a9-2101710e7236   \n",
       "4  445013be-57fb-4da3-904d-dae3919068cb  3c280879-e26e-4c37-92fa-413a2a4ebafd   \n",
       "\n",
       "   event  \n",
       "0      0  \n",
       "1      0  \n",
       "2      1  \n",
       "3      0  \n",
       "4      1  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import uuid\n",
    "\n",
    "event = []\n",
    "player_id = []\n",
    "session_id = []\n",
    "\n",
    "for x in range(5):\n",
    "    event.append(int(random.uniform(0,2)))\n",
    "    player_id.append(uuid.uuid4())\n",
    "    session_id.append(uuid.uuid4())\n",
    "df1 = pd.DataFrame(zip(player_id, session_id, event), columns=['player_id', 'session_id', 'event'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9294131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro\n",
    "import avro.schema\n",
    "import json\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "import fastavro \n",
    "from fastavro import writer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "adf6768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'doc': 'A weather reading.',\n",
    "    'name': 'Weather',\n",
    "    'namespace': 'test',\n",
    "    'type': 'record',\n",
    "    'fields': [\n",
    "        {'name': 'station', 'type': 'string'},\n",
    "        {'name': 'time', 'type': 'long'},\n",
    "        {'name': 'temp', 'type': 'int'},\n",
    "    ],\n",
    "}\n",
    "\n",
    "records = [\n",
    "    {'station': '011990-99999', 'temp': 0, 'time': 1433269388},\n",
    "    {'station': '011990-99999', 'temp': 22, 'time': 1433270389},\n",
    "    {'station': '011990-99999', 'temp': -11, 'time': 1433273379},\n",
    "    {'station': '012650-99999', 'temp': 111, 'time': 1433275478},\n",
    "]\n",
    "\n",
    "with open('weather.avro', 'wb') as out:\n",
    "    writer(out, schema, records)\n",
    "schema_parsed = avro.schema.parse(json.dumps(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "48c42505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema that we specified:\n",
      " StructType([StructField('player_id', IntegerType(), True), StructField('session_id', StringType(), True), StructField('event', StringType(), True)])\n",
      "Schema that we parsed:\n",
      " {\"type\": \"record\", \"name\": \"Weather\", \"namespace\": \"test\", \"fields\": [{\"type\": \"string\", \"name\": \"station\"}, {\"type\": \"long\", \"name\": \"time\"}, {\"type\": \"int\", \"name\": \"temp\"}], \"doc\": \"A weather reading.\"}\n",
      "Schema from users.avro file:\n",
      " {'type': 'record', 'name': 'Weather', 'namespace': 'test', 'fields': [{'type': 'string', 'name': 'station'}, {'type': 'long', 'name': 'time'}, {'type': 'int', 'name': 'temp'}], 'doc': 'A weather reading.'}\n",
      "Stations:\n",
      " [{'station': '012334', 'time': 1323434, 'temp': 77}]\n"
     ]
    }
   ],
   "source": [
    "with open('weather.avro', 'wb') as f:\n",
    "    writer = DataFileWriter(f, DatumWriter(), schema_parsed)\n",
    "    writer.append({'station': '012334', 'temp': 77, 'time': 1323434})\n",
    "    writer.close()\n",
    "\n",
    "with open('weather.avro', 'rb') as f:\n",
    "    reader = DataFileReader(f, DatumReader())\n",
    "    metadata = copy.deepcopy(reader.meta)\n",
    "    schema_from_file = json.loads(metadata['avro.schema'])\n",
    "    station = [station for station in reader]\n",
    "    reader.close()\n",
    "\n",
    "print(f'Schema that we specified:\\n {schema}')\n",
    "print(f'Schema that we parsed:\\n {schema_parsed}')\n",
    "print(f'Schema from users.avro file:\\n {schema_from_file}')\n",
    "print(f'Stations:\\n {station}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1286151",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install kaggle \n",
    "! export KAGGLE_USERNAME= my_name\n",
    "! export KAGGLE_KEY= my_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f430c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data-science-job-salaries.zip to /Users/ekaterinaakulova/DataScience\n",
      "  0%|                                               | 0.00/7.37k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 7.37k/7.37k [00:00<00:00, 2.62MB/s]\n",
      "Archive:  data-science-job-salaries.zip\n",
      "replace ds_salaries.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "! kaggle datasets download  --force -d ruchi798/data-science-job-salaries\n",
    "! unzip data-science-job-salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0f1eb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.32 ms, sys: 4.66 ms, total: 13 ms\n",
      "Wall time: 16.9 ms\n"
     ]
    }
   ],
   "source": [
    "pd_df = pd.read_csv('ds_salaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0c4845ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 632 µs, sys: 2.56 ms, total: 3.19 ms\n",
      "Wall time: 5.33 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>70000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>79833</td>\n",
       "      <td>DE</td>\n",
       "      <td>0</td>\n",
       "      <td>DE</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Scientist</td>\n",
       "      <td>260000</td>\n",
       "      <td>USD</td>\n",
       "      <td>260000</td>\n",
       "      <td>JP</td>\n",
       "      <td>0</td>\n",
       "      <td>JP</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>85000</td>\n",
       "      <td>GBP</td>\n",
       "      <td>109024</td>\n",
       "      <td>GB</td>\n",
       "      <td>50</td>\n",
       "      <td>GB</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Product Data Analyst</td>\n",
       "      <td>20000</td>\n",
       "      <td>USD</td>\n",
       "      <td>20000</td>\n",
       "      <td>HN</td>\n",
       "      <td>0</td>\n",
       "      <td>HN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>150000</td>\n",
       "      <td>USD</td>\n",
       "      <td>150000</td>\n",
       "      <td>US</td>\n",
       "      <td>50</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  work_year experience_level employment_type  \\\n",
       "0           0       2020               MI              FT   \n",
       "1           1       2020               SE              FT   \n",
       "2           2       2020               SE              FT   \n",
       "3           3       2020               MI              FT   \n",
       "4           4       2020               SE              FT   \n",
       "\n",
       "                    job_title  salary salary_currency  salary_in_usd  \\\n",
       "0              Data Scientist   70000             EUR          79833   \n",
       "1  Machine Learning Scientist  260000             USD         260000   \n",
       "2           Big Data Engineer   85000             GBP         109024   \n",
       "3        Product Data Analyst   20000             USD          20000   \n",
       "4   Machine Learning Engineer  150000             USD         150000   \n",
       "\n",
       "  employee_residence  remote_ratio company_location company_size  \n",
       "0                 DE             0               DE            L  \n",
       "1                 JP             0               JP            S  \n",
       "2                 GB            50               GB            M  \n",
       "3                 HN             0               HN            S  \n",
       "4                 US            50               US            L  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bb2dc3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.16 ms, sys: 3.1 ms, total: 6.26 ms\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'ds_salaries.csv'\n",
    "df_csv_ss = ss.read.load(csv_file,\n",
    "                     format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\") #spark csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "488239a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.88 ms, sys: 5.98 ms, total: 11.9 ms\n",
      "Wall time: 202 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(,work_year,experience_level,employment_type,job_title,salary,salary_currency,salary_in_usd,employee_residence,remote_ratio,company_location,company_size='0,2020,MI,FT,Data Scientist,70000,EUR,79833,DE,0,DE,L'),\n",
       " Row(,work_year,experience_level,employment_type,job_title,salary,salary_currency,salary_in_usd,employee_residence,remote_ratio,company_location,company_size='1,2020,SE,FT,Machine Learning Scientist,260000,USD,260000,JP,0,JP,S'),\n",
       " Row(,work_year,experience_level,employment_type,job_title,salary,salary_currency,salary_in_usd,employee_residence,remote_ratio,company_location,company_size='2,2020,SE,FT,Big Data Engineer,85000,GBP,109024,GB,50,GB,M'),\n",
       " Row(,work_year,experience_level,employment_type,job_title,salary,salary_currency,salary_in_usd,employee_residence,remote_ratio,company_location,company_size='3,2020,MI,FT,Product Data Analyst,20000,USD,20000,HN,0,HN,S'),\n",
       " Row(,work_year,experience_level,employment_type,job_title,salary,salary_currency,salary_in_usd,employee_residence,remote_ratio,company_location,company_size='4,2020,SE,FT,Machine Learning Engineer,150000,USD,150000,US,50,US,L')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv_ss.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "c32fb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.to_parquet('ds_salaries.parquet') #Parquet — формат файлов,\n",
    "#который хранит вложенные структуры данных в плоском столбчатом формате.\n",
    "#Столбчатый формат более эффективен, когда вам нужно запросить из таблицы несколько столбцов. \n",
    "#Он прочитает только необходимые столбцы, потому что они находятся по соседству. Операции ввода-вывода сводятся к минимуму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9697fc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 ms, sys: 2.95 ms, total: 6.86 ms\n",
      "Wall time: 87.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Unnamed: 0=0, work_year=2020, experience_level='MI', employment_type='FT', job_title='Data Scientist', salary=70000, salary_currency='EUR', salary_in_usd=79833, employee_residence='DE', remote_ratio=0, company_location='DE', company_size='L'),\n",
       " Row(Unnamed: 0=1, work_year=2020, experience_level='SE', employment_type='FT', job_title='Machine Learning Scientist', salary=260000, salary_currency='USD', salary_in_usd=260000, employee_residence='JP', remote_ratio=0, company_location='JP', company_size='S'),\n",
       " Row(Unnamed: 0=2, work_year=2020, experience_level='SE', employment_type='FT', job_title='Big Data Engineer', salary=85000, salary_currency='GBP', salary_in_usd=109024, employee_residence='GB', remote_ratio=50, company_location='GB', company_size='M'),\n",
       " Row(Unnamed: 0=3, work_year=2020, experience_level='MI', employment_type='FT', job_title='Product Data Analyst', salary=20000, salary_currency='USD', salary_in_usd=20000, employee_residence='HN', remote_ratio=0, company_location='HN', company_size='S'),\n",
       " Row(Unnamed: 0=4, work_year=2020, experience_level='SE', employment_type='FT', job_title='Machine Learning Engineer', salary=150000, salary_currency='USD', salary_in_usd=150000, employee_residence='US', remote_ratio=50, company_location='US', company_size='L')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.head(5) #формат данных parquet считывается быстрее "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "4ebe2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.57 ms, sys: 2.41 ms, total: 5.99 ms\n",
      "Wall time: 408 ms\n"
     ]
    }
   ],
   "source": [
    "df_parquet = ss.read.load('ds_salaries.parquet')\n",
    "df_parquet.select(\"job_title\", \"salary_in_usd\").write.save(\"salaries_jobs.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0874a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.45 ms, sys: 14.9 ms, total: 22.3 ms\n",
      "Wall time: 1.39 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(job_title='Data Scientist', salary_in_usd=79833),\n",
       " Row(job_title='Machine Learning Scientist', salary_in_usd=260000),\n",
       " Row(job_title='Big Data Engineer', salary_in_usd=109024),\n",
       " Row(job_title='Product Data Analyst', salary_in_usd=20000),\n",
       " Row(job_title='Machine Learning Engineer', salary_in_usd=150000)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ss.read.parquet(\"salaries_jobs.parquet\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "00e44f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: string (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n",
      "CPU times: user 3.12 ms, sys: 2.3 ms, total: 5.42 ms\n",
      "Wall time: 325 ms\n"
     ]
    }
   ],
   "source": [
    "spark_df = ss.read.option('header', True).csv('ds_salaries.csv')\n",
    "spark_df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c9d31c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 ms, sys: 2.17 ms, total: 4.32 ms\n",
      "Wall time: 129 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(607, 12)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count(), len(spark_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "5af16713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 ms, sys: 2.16 ms, total: 5.97 ms\n",
      "Wall time: 21.2 ms\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark_df.select(\"work_year\", \"job_title\", \"salary_in_usd\", \"company_location\", \"company_size\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d4156c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = [\n",
    "    ('spark.driver.memory', '4g'), #выделяем 4гб оперативной памяти процессу\n",
    "    ('spark.driver.maxResultSize', '2g'), #исполнители будут давать результаты размером макс 2 гь\n",
    "    ('spark.executor.memory', '1g'), #1гб занимает каждый исполнитель\n",
    "    ('spark.executor.instances', '2'), #количество исполнителей\n",
    "    ('spark.dinamicAlocation.enabled', 'false'), #флаг отвечает за динамическое распределение ресурсов\n",
    "    ('spark.default.parallelism', '8'), #на сколько частей будут разбиваться наборы данных чтобы работать с ними параллельно\n",
    "    ('spark.eventlog.enabled', 'false') #более делально смотрит за работой спарка и читает логи\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, HiveContext, SQLContext\n",
    "\n",
    "conf = SparkConf().setAll(cfg)\n",
    "ss.stop() \n",
    "sc = SparkContext(appName = 'example', conf = conf)\n",
    "ss = SparkSession(sc).builder.enableHiveSupport().getOrCreate()\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d77d536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/29 13:53:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      " Schema: _c0, work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/ekaterinaakulova/DataScience/ds_salaries.csv\n",
      "22/08/29 13:53:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      " Schema: _c0, work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/ekaterinaakulova/DataScience/ds_salaries.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>work_year</th><th>experience_level</th><th>employment_type</th><th>job_title</th><th>salary</th><th>salary_currency</th><th>salary_in_usd</th><th>employee_residence</th><th>remote_ratio</th><th>company_location</th><th>company_size</th></tr>\n",
       "<tr><td>289</td><td>2022</td><td>SE</td><td>FT</td><td>Data Engineer</td><td>135000</td><td>USD</td><td>135000</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>290</td><td>2022</td><td>SE</td><td>FT</td><td>Data Analyst</td><td>155000</td><td>USD</td><td>155000</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>291</td><td>2022</td><td>SE</td><td>FT</td><td>Data Analyst</td><td>120600</td><td>USD</td><td>120600</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>292</td><td>2022</td><td>MI</td><td>FT</td><td>Data Scientist</td><td>130000</td><td>USD</td><td>130000</td><td>US</td><td>0</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>293</td><td>2022</td><td>MI</td><td>FT</td><td>Data Scientist</td><td>90000</td><td>USD</td><td>90000</td><td>US</td><td>0</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>294</td><td>2022</td><td>MI</td><td>FT</td><td>Data Engineer</td><td>170000</td><td>USD</td><td>170000</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>295</td><td>2022</td><td>MI</td><td>FT</td><td>Data Engineer</td><td>150000</td><td>USD</td><td>150000</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>296</td><td>2022</td><td>SE</td><td>FT</td><td>Data Analyst</td><td>102100</td><td>USD</td><td>102100</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>297</td><td>2022</td><td>SE</td><td>FT</td><td>Data Analyst</td><td>84900</td><td>USD</td><td>84900</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>298</td><td>2022</td><td>SE</td><td>FT</td><td>Data Scientist</td><td>136620</td><td>USD</td><td>136620</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>299</td><td>2022</td><td>SE</td><td>FT</td><td>Data Scientist</td><td>99360</td><td>USD</td><td>99360</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>300</td><td>2022</td><td>SE</td><td>FT</td><td>Data Scientist</td><td>90000</td><td>GBP</td><td>117789</td><td>GB</td><td>0</td><td>GB</td><td>M</td></tr>\n",
       "<tr><td>301</td><td>2022</td><td>SE</td><td>FT</td><td>Data Scientist</td><td>80000</td><td>GBP</td><td>104702</td><td>GB</td><td>0</td><td>GB</td><td>M</td></tr>\n",
       "<tr><td>302</td><td>2022</td><td>SE</td><td>FT</td><td>Data Scientist</td><td>146000</td><td>USD</td><td>146000</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>303</td><td>2022</td><td>SE</td><td>FT</td><td>Data Scientist</td><td>123000</td><td>USD</td><td>123000</td><td>US</td><td>100</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>304</td><td>2022</td><td>EN</td><td>FT</td><td>Data Engineer</td><td>40000</td><td>GBP</td><td>52351</td><td>GB</td><td>100</td><td>GB</td><td>M</td></tr>\n",
       "<tr><td>305</td><td>2022</td><td>SE</td><td>FT</td><td>Data Analyst</td><td>99000</td><td>USD</td><td>99000</td><td>US</td><td>0</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>306</td><td>2022</td><td>SE</td><td>FT</td><td>Data Analyst</td><td>116000</td><td>USD</td><td>116000</td><td>US</td><td>0</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>307</td><td>2022</td><td>MI</td><td>FT</td><td>Data Analyst</td><td>106260</td><td>USD</td><td>106260</td><td>US</td><td>0</td><td>US</td><td>M</td></tr>\n",
       "<tr><td>308</td><td>2022</td><td>MI</td><td>FT</td><td>Data Analyst</td><td>126500</td><td>USD</td><td>126500</td><td>US</td><td>0</td><td>US</td><td>M</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+---------+----------------+---------------+--------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
       "|_c0|work_year|experience_level|employment_type|     job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
       "+---+---------+----------------+---------------+--------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
       "|289|     2022|              SE|             FT| Data Engineer|135000|            USD|       135000|                US|         100|              US|           M|\n",
       "|290|     2022|              SE|             FT|  Data Analyst|155000|            USD|       155000|                US|         100|              US|           M|\n",
       "|291|     2022|              SE|             FT|  Data Analyst|120600|            USD|       120600|                US|         100|              US|           M|\n",
       "|292|     2022|              MI|             FT|Data Scientist|130000|            USD|       130000|                US|           0|              US|           M|\n",
       "|293|     2022|              MI|             FT|Data Scientist| 90000|            USD|        90000|                US|           0|              US|           M|\n",
       "|294|     2022|              MI|             FT| Data Engineer|170000|            USD|       170000|                US|         100|              US|           M|\n",
       "|295|     2022|              MI|             FT| Data Engineer|150000|            USD|       150000|                US|         100|              US|           M|\n",
       "|296|     2022|              SE|             FT|  Data Analyst|102100|            USD|       102100|                US|         100|              US|           M|\n",
       "|297|     2022|              SE|             FT|  Data Analyst| 84900|            USD|        84900|                US|         100|              US|           M|\n",
       "|298|     2022|              SE|             FT|Data Scientist|136620|            USD|       136620|                US|         100|              US|           M|\n",
       "|299|     2022|              SE|             FT|Data Scientist| 99360|            USD|        99360|                US|         100|              US|           M|\n",
       "|300|     2022|              SE|             FT|Data Scientist| 90000|            GBP|       117789|                GB|           0|              GB|           M|\n",
       "|301|     2022|              SE|             FT|Data Scientist| 80000|            GBP|       104702|                GB|           0|              GB|           M|\n",
       "|302|     2022|              SE|             FT|Data Scientist|146000|            USD|       146000|                US|         100|              US|           M|\n",
       "|303|     2022|              SE|             FT|Data Scientist|123000|            USD|       123000|                US|         100|              US|           M|\n",
       "|304|     2022|              EN|             FT| Data Engineer| 40000|            GBP|        52351|                GB|         100|              GB|           M|\n",
       "|305|     2022|              SE|             FT|  Data Analyst| 99000|            USD|        99000|                US|           0|              US|           M|\n",
       "|306|     2022|              SE|             FT|  Data Analyst|116000|            USD|       116000|                US|           0|              US|           M|\n",
       "|307|     2022|              MI|             FT|  Data Analyst|106260|            USD|       106260|                US|           0|              US|           M|\n",
       "|308|     2022|              MI|             FT|  Data Analyst|126500|            USD|       126500|                US|           0|              US|           M|\n",
       "+---+---------+----------------+---------------+--------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "spark_df.filter((col('work_year') == lit('2022')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4bf05377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+----+-----------------+---------------------+\n",
      "|experience_level|from|  to|min salary in usd|average salary in usd|\n",
      "+----------------+----+----+-----------------+---------------------+\n",
      "|              EN|2020|2022|            10000|   61643.318181818184|\n",
      "|              EX|2020|2022|           110000|   199392.03846153847|\n",
      "|              MI|2020|2022|           100000|    87996.05633802817|\n",
      "|              SE|2020|2022|           100000|   138617.29285714286|\n",
      "+----------------+----+----+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark_df.filter((col('work_year') >= lit('2020')) & (col('work_year') <= lit('2022')))\\\n",
    "    .groupBy(\"experience_level\") \\\n",
    "    .agg(f.min(\"work_year\").alias(\"from\"), \n",
    "         f.max(\"work_year\").alias(\"to\"), \n",
    "         \n",
    "         f.min(\"salary_in_usd\").alias(\"min salary in usd\"),\n",
    "         f.avg(\"salary_in_usd\").alias(\"average salary in usd\")\n",
    "      ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b8aeb755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 729 µs, sys: 245 µs, total: 974 µs\n",
      "Wall time: 936 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column<'company_location'>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = spark_df['company_location']\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ba73148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "type(spark_df.salary_in_usd) == type(spark_df.salary_in_usd + 1) == type(spark_df.salary_in_usd.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ee8a53b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-----------------+\n",
      "|company_location|salary_in_usd|salary_in_usd + 1|\n",
      "+----------------+-------------+-----------------+\n",
      "|              DE|        79833|          79834.0|\n",
      "|              JP|       260000|         260001.0|\n",
      "|              GB|       109024|         109025.0|\n",
      "|              HN|        20000|          20001.0|\n",
      "|              US|       150000|         150001.0|\n",
      "|              US|        72000|          72001.0|\n",
      "|              US|       190000|         190001.0|\n",
      "|              HU|        35735|          35736.0|\n",
      "|              US|       135000|         135001.0|\n",
      "|              NZ|       125000|         125001.0|\n",
      "|              FR|        51321|          51322.0|\n",
      "|              IN|        40481|          40482.0|\n",
      "|              FR|        39916|          39917.0|\n",
      "|              US|        87000|          87001.0|\n",
      "|              US|        85000|          85001.0|\n",
      "|              PK|         8000|           8001.0|\n",
      "|              JP|        41689|          41690.0|\n",
      "|              GB|       114047|         114048.0|\n",
      "|              IN|         5707|           5708.0|\n",
      "|              US|        56000|          56001.0|\n",
      "+----------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df_0 = spark_df.select('company_location', 'salary_in_usd')\n",
    "spark_df_0.withColumn('salary_in_usd + 1', spark_df_0.salary_in_usd + 1).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "eea24cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>company_location</th><th>salary_in_usd</th></tr>\n",
       "<tr><td>RU</td><td>230000</td></tr>\n",
       "<tr><td>RU</td><td>85000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------+-------------+\n",
       "|company_location|salary_in_usd|\n",
       "+----------------+-------------+\n",
       "|              RU|       230000|\n",
       "|              RU|        85000|\n",
       "+----------------+-------------+"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_0.filter(spark_df.company_location == 'RU') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
